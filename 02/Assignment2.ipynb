{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:\n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>275</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31</td>\n",
              "      <td>a</td>\n",
              "      <td>aaa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>a</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>a</td>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>192</td>\n",
              "      <td>a</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020380</th>\n",
              "      <td>24</td>\n",
              "      <td>zviad</td>\n",
              "      <td>gamsakhurdia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020381</th>\n",
              "      <td>25</td>\n",
              "      <td>zweimal</td>\n",
              "      <td>leben</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020382</th>\n",
              "      <td>24</td>\n",
              "      <td>zwick</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020383</th>\n",
              "      <td>24</td>\n",
              "      <td>zydeco</td>\n",
              "      <td>music</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020384</th>\n",
              "      <td>72</td>\n",
              "      <td>zz</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020378 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0        1             2\n",
              "0        275        a             a\n",
              "1         31        a           aaa\n",
              "2         29        a           all\n",
              "3         45        a            an\n",
              "4        192        a           and\n",
              "...      ...      ...           ...\n",
              "1020380   24    zviad  gamsakhurdia\n",
              "1020381   25  zweimal         leben\n",
              "1020382   24    zwick           and\n",
              "1020383   24   zydeco         music\n",
              "1020384   72       zz           top\n",
              "\n",
              "[1020378 rows x 3 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_data = pd.read_csv(\"bigram_data.txt\", sep=\"\\t\", header=None, on_bad_lines=\"skip\")\n",
        "bigram_data = bigram_data.dropna()\n",
        "bigram_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('a', 'a') 275\n",
            "('a', 'aaa') 31\n",
            "('a', 'all') 29\n",
            "('a', 'an') 45\n",
            "('a', 'and') 192\n",
            "('a', 'another') 39\n",
            "('a', 'at') 25\n",
            "('a', 'b') 82\n",
            "('a', 'b+') 45\n",
            "('a', 'b-17') 26\n"
          ]
        }
      ],
      "source": [
        "# Convert to dictionary of (word1, word2): count\n",
        "BIGRAM_COUNTER = bigram_data.set_index([1, 2]).to_dict()[0]\n",
        "BIGRAM_COUNTER = defaultdict(int, BIGRAM_COUNTER)\n",
        "\n",
        "BIGRAM_COUNTER_TOTAL = sum(BIGRAM_COUNTER.values())\n",
        "\n",
        "# Preview\n",
        "for item in list(BIGRAM_COUNTER.items())[:10]:\n",
        "    print(item[0], item[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a: 23832\n",
            "aaa: 12\n",
            "all: 4034\n",
            "an: 5740\n",
            "and: 52475\n",
            "another: 1857\n",
            "at: 10127\n",
            "b: 167\n",
            "b+: 1\n",
            "b-17: 2\n"
          ]
        }
      ],
      "source": [
        "# Convert to dictionary of word: count\n",
        "VOCAB = defaultdict(int)\n",
        "\n",
        "for word1, word2 in BIGRAM_COUNTER.keys():\n",
        "    VOCAB[word1] += 1\n",
        "    VOCAB[word2] += 1\n",
        "\n",
        "VOCAB_TOTAL = sum(VOCAB.values())\n",
        "\n",
        "# Preview\n",
        "for item in list(VOCAB.items())[:10]:\n",
        "    print(f\"{item[0]}: {item[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def known_words(words: list[str] | set[str]) -> set[str]:\n",
        "    \"\"\"Filters out words that are not in the vocabulary.\"\"\"\n",
        "    return set(w for w in words if VOCAB[w] > 0)\n",
        "\n",
        "\n",
        "def get_word_prob(word: str) -> float:\n",
        "    \"\"\"Returns the probability of `word` in the corpus.\"\"\"\n",
        "    return VOCAB[word] / VOCAB_TOTAL\n",
        "\n",
        "\n",
        "def get_bigram_prob(bigram: tuple[str, str]) -> float:\n",
        "    \"\"\"Returns the probability of `bigram` in the corpus.\"\"\"\n",
        "    return BIGRAM_COUNTER[bigram] / BIGRAM_COUNTER_TOTAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import lru_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=None)\n",
        "def get_words_at_edit_distance_n(word: str, n: int) -> set[str]:\n",
        "    \"\"\"Returns the set of all words at edit distance `n` from `word`.\"\"\"\n",
        "    if n <= 0:\n",
        "        return {word}\n",
        "\n",
        "    if n == 1:\n",
        "        letters    = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    else:\n",
        "        out = set()\n",
        "\n",
        "        for word_ in get_words_at_edit_distance_n(word, n - 1):\n",
        "            out |= get_words_at_edit_distance_n(word_, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def get_word_candidates(word: str, edit_distance: int = 1) -> set[str]:\n",
        "    \"\"\"Returns candidates for a word at a given edit distance.\"\"\"\n",
        "    candidates = known_words([word])\n",
        "\n",
        "    n = 1\n",
        "    while not candidates and n <= edit_distance:\n",
        "        candidates |= known_words(get_words_at_edit_distance_n(word, n))\n",
        "        n += 1\n",
        "\n",
        "    if not candidates:\n",
        "        return {word}\n",
        "\n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_unigram_suggestions(\n",
        "    word: str, edit_distance: int = 1\n",
        ") -> list[tuple[str, float]]:\n",
        "    \"\"\"Returns a list of suggested words based on unigram probabilities.\"\"\"\n",
        "    word_candidates = get_word_candidates(word, edit_distance)\n",
        "\n",
        "    suggestions = []\n",
        "    for word in word_candidates:\n",
        "        prob = get_word_prob(word)\n",
        "        if prob > 0:\n",
        "            suggestions.append((word, prob))\n",
        "\n",
        "    return sorted(suggestions, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "def get_bigram_suggestions(\n",
        "    bigram: tuple[str, str], edit_distance: int = 1, left=True\n",
        ") -> list[tuple[tuple[str, str], float]]:\n",
        "    \"\"\"Returns a list of suggested bigrams based on bigram probabilities.\"\"\"\n",
        "    word_candidates = get_word_candidates(bigram[0 if left else 1], edit_distance)\n",
        "\n",
        "    candidate_bigrams = set()\n",
        "    for word in word_candidates:\n",
        "        if left:\n",
        "            candidate_bigrams.add((word, bigram[1]))\n",
        "        else:\n",
        "            candidate_bigrams.add((bigram[0], word))\n",
        "\n",
        "    suggestions = []\n",
        "    for c in candidate_bigrams:\n",
        "        prob = get_bigram_prob(c)\n",
        "        if prob > 0:\n",
        "            suggestions.append((c, prob))\n",
        "\n",
        "    return sorted(suggestions, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autocorrect(\n",
        "    sentence: str,\n",
        "    edit_distance: int = 1,\n",
        "    vocab_word_min_prob: float = 1e-6,\n",
        "    non_vocab_word_min_prob: float = 1e-15,\n",
        ") -> str:\n",
        "    \"\"\"Returns the corrected sentence based on bigram probabilities.\"\"\"\n",
        "    words = sentence.split()\n",
        "\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    sentence_corrected = []\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == 0:\n",
        "            right_suggestions = []\n",
        "        else:\n",
        "            right_suggestions = get_bigram_suggestions(\n",
        "                (words[i - 1], words[i]), edit_distance, left=False\n",
        "            )\n",
        "            # Convert ((a, b), p) to (a, p)\n",
        "            right_suggestions = [(w, p) for ((_, w), p) in right_suggestions]\n",
        "\n",
        "        if i == n - 1:\n",
        "            left_suggestions = []\n",
        "        else:\n",
        "            left_suggestions = get_bigram_suggestions(\n",
        "                (words[i], words[i + 1]), edit_distance, left=True\n",
        "            )\n",
        "            # Convert ((a, b), p) to (b, p)\n",
        "            left_suggestions = [(w, p) for ((w, _), p) in left_suggestions]\n",
        "\n",
        "        suggestions = left_suggestions + right_suggestions\n",
        "\n",
        "        # No bigram suggestions, try unigrams if the word is not in the vocabulary\n",
        "        known_word = known_words([words[i]])\n",
        "\n",
        "        if len(suggestions) == 0 and not known_word:\n",
        "            suggestions = get_unigram_suggestions(words[i], edit_distance)\n",
        "\n",
        "        # No suggestions, keep the word\n",
        "        if len(suggestions) == 0:\n",
        "            sentence_corrected.append(words[i])\n",
        "            continue\n",
        "\n",
        "        # Pick the word by the max sum of unigram and bigram probabilities\n",
        "        probs = defaultdict(lambda: (0, 0))\n",
        "        for w, p in suggestions:\n",
        "            prob, count = probs[w]\n",
        "            probs[w] = (prob + p, count + 1)\n",
        "\n",
        "        # Average the probabilities\n",
        "        for w, (p, c) in probs.items():\n",
        "            probs[w] = p / c\n",
        "\n",
        "        # Filter out very low probability words\n",
        "        probs = {\n",
        "            w: p\n",
        "            for w, p in probs.items()\n",
        "            if p > (vocab_word_min_prob if known_word else non_vocab_word_min_prob)\n",
        "        }\n",
        "\n",
        "        # Add max probability word to the corrected sentence\n",
        "        if len(probs) == 0:\n",
        "            sentence_corrected.append(words[i])\n",
        "        else:\n",
        "            sentence_corrected.append(max(probs, key=probs.get))\n",
        "\n",
        "    return \" \".join(sentence_corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NORGIG'S SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norvig_correction(word: str, edit_distance: int = 1) -> str:\n",
        "    candidates = get_word_candidates(word, edit_distance)\n",
        "    return max(candidates, key=get_word_prob)\n",
        "\n",
        "\n",
        "def norvig_autocorrect(sentence: str, edit_distance: int = 1) -> str:\n",
        "    sentence_corrected = []\n",
        "\n",
        "    for word in sentence.split():\n",
        "        sentence_corrected.append(norvig_correction(word, edit_distance))\n",
        "\n",
        "    return \" \".join(sentence_corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spelling correctly\n"
          ]
        }
      ],
      "source": [
        "print(norvig_autocorrect(\"speling korrectly\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:          i like cokking crystal mth\n",
            "Corrected:         i like cooking crystal meth\n",
            "Norvig Corrected:  i like cooking crystal math\n",
            "\n",
            "Original:          i lke solvng mth prblems\n",
            "Corrected:         i like solving math problems\n",
            "Norvig Corrected:  i like solving math problems\n",
            "\n",
            "Original:          dking sport\n",
            "Corrected:         doing sport\n",
            "Norvig Corrected:  doing sport\n",
            "\n",
            "Original:          dking patient\n",
            "Corrected:         dying patient\n",
            "Norvig Corrected:  doing patient\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLES (by Polina Zelenskaya)\n",
        "examples = [\n",
        "    \"i like cokking crystal mth\",\n",
        "    \"i lke solvng mth prblems\",\n",
        "    \"dking sport\",\n",
        "    \"dking patient\",\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    print(f\"Original:{'':>10}{example}\")\n",
        "    print(f\"Corrected:{'':>9}{autocorrect(example)}\")\n",
        "    print(f\"Norvig Corrected:{'':>2}{norvig_autocorrect(example)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "**Which ngram dataset to use.**\n",
        "\n",
        "I chose to use the provided bigram data (bigram_data.txt) as it is a reasonably big dataset containing over one million bigram counts from a English corpus. Using trigrams or higher n-grams would provide more context but would also significantly increase the search space.\n",
        "\n",
        "**Word probability calculation.**\n",
        "\n",
        "For unigram word probabilities, I simply used the maximum likelihood estimate\n",
        "$$P(word) = \\frac{count(word)}{total\\_words}.$$\n",
        "\n",
        "**Bigram probability calculation.**\n",
        "Similarly, for bigram probabilities $P(word2 | word1)$, I used the maximum likelihood estimate\n",
        "$$\\frac{P(word1, word2)}{sum\\_w(P(word1, w))}.$$\n",
        "\n",
        "**Edit distance candidates.**\n",
        "\n",
        "To generate candidate words for a given word, using Norvig's solution as a basis, I first check if the word itself is in the vocabulary. If not, I generate words within increasing edit distances (1, 2, 3, ...) from the word until at least one candidate is found that exists in the vocabulary.\n",
        "\n",
        "**Combining unigram and bigram suggestions.**\n",
        "\n",
        "For a given word in a sentence, I generate left and right bigrams, changing only the central word. The bigram probabilities are combined in a sum to rank the suggestions. If no suggestions made using bigrams, unigrams suggestions are used instead. Very low probability suggestions are filtered out using tunable minimum probability thresholds.\n",
        "\n",
        "**No beam search.**\n",
        "\n",
        "For this assignment, I opted for a more straightforward approach that demonstrates the core concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "class ConfusionMatrix:\n",
        "    def __init__(self, TP: int = 0, FP: int = 0, TN: int = 0, FN: int = 0):\n",
        "        self.TP = TP\n",
        "        self.FP = FP\n",
        "        self.TN = TN\n",
        "        self.FN = FN\n",
        "\n",
        "    def accuracy(self) -> float:\n",
        "        num = self.TP + self.TN\n",
        "        if num == 0:\n",
        "            return 0\n",
        "\n",
        "        return num / (self.TP + self.FP + self.TN + self.FN)\n",
        "\n",
        "    def precision(self) -> float:\n",
        "        if self.TP == 0:\n",
        "            return 0\n",
        "\n",
        "        return self.TP / (self.TP + self.FP)\n",
        "\n",
        "    def recall(self) -> float:\n",
        "        if self.TP == 0:\n",
        "            return 0\n",
        "\n",
        "        return self.TP / (self.TP + self.FN)\n",
        "\n",
        "    def f1(self) -> float:\n",
        "        p = self.precision()\n",
        "        r = self.recall()\n",
        "\n",
        "        if p + r == 0:\n",
        "            return 0\n",
        "\n",
        "        return 2 * p * r / (p + r)\n",
        "\n",
        "    def __add__(self, other: \"ConfusionMatrix\") -> \"ConfusionMatrix\":\n",
        "        return ConfusionMatrix(\n",
        "            self.TP + other.TP,\n",
        "            self.FP + other.FP,\n",
        "            self.TN + other.TN,\n",
        "            self.FN + other.FN,\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"\"\"\\\n",
        "        | {'':->10} | {'':->10} |\n",
        "        | {'TP':>10} | {self.TP:>10} |\n",
        "        | {'FP':>10} | {self.FP:>10} |\n",
        "        | {'TN':>10} | {self.TN:>10} |\n",
        "        | {'FN':>10} | {self.FN:>10} |\n",
        "        | {'':->10} | {'':->10} |\n",
        "        | {'Accuracy':>10} | {self.accuracy():>10.2f} |\n",
        "        | {'Precision':>10} | {self.precision():>10.2f} |\n",
        "        | {'Recall':>10} | {self.recall():>10.2f} |\n",
        "        | {'F1':>10} | {self.f1():>10.2f} |\n",
        "        | {'':->10} | {'':->10} |\n",
        "        \"\"\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_correction_scores(\n",
        "    text_original: str, text_mutated: str, text_corrected: str\n",
        ") -> ConfusionMatrix:\n",
        "    words_original = text_original.split()\n",
        "    words_mutated = text_mutated.split()\n",
        "    words_corrected = text_corrected.split()\n",
        "\n",
        "    confusion_matrix = ConfusionMatrix()\n",
        "\n",
        "    for i in range(len(words_original)):\n",
        "        if words_original[i] == words_mutated[i]:  # NO ERROR\n",
        "            if words_original[i] == words_corrected[i]:\n",
        "                confusion_matrix.TN += 1  # CORRECTLY IGNORED\n",
        "            else:\n",
        "                confusion_matrix.FP += 1  # INCORRECTLY CHANGED\n",
        "        else:  # THERE IS AN ERROR\n",
        "            if words_original[i] == words_corrected[i]:\n",
        "                confusion_matrix.TP += 1  # CORRECTLY FIXED\n",
        "            else:\n",
        "                confusion_matrix.FN += 1  # INCORRECTLY IGNORED\n",
        "\n",
        "    return confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mutate_random_words_in_text(text: str, num: int, edit_distance: int = 1) -> str:\n",
        "    \"\"\"Randomly changes `num` words in the `text` to words with edit distance `edit_distance`.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return text\n",
        "\n",
        "    n = len(words)\n",
        "    for i in random.sample(range(n), min(n, num)):\n",
        "        variants = get_words_at_edit_distance_n(words[i], edit_distance)\n",
        "        variants = [v for v in variants if v]\n",
        "\n",
        "        if variants:\n",
        "            words[i] = random.choice(variants)\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Preprocesses the text for autocorrection.\"\"\"\n",
        "    # Remove all non-alphabetic characters and convert to lowercase\n",
        "    text = \"\".join([c for c in text if c.isalpha() or c.isspace()])\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       confused and frustrated connie decides to leav...\n",
              "1          later a womans scream is heard in the distance\n",
              "2                 christian is then paralyzed by an elder\n",
              "3                               the temple is set on fire\n",
              "4                         outside the cult wails with him\n",
              "                              ...                        \n",
              "4313    confidencial also responded negatively calling...\n",
              "4314    and le parisien gave the film their highest fi...\n",
              "4315    the museum collection includes  film titles  p...\n",
              "4316    its predecessor was the dutch historical film ...\n",
              "4317            sfilmstar greta garbo by alexander binder\n",
              "Name: sentence, Length: 4318, dtype: object"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the data\n",
        "data_test = pd.read_csv(\"wiki_sentences_v2.csv\", sep=\"\\t\")\n",
        "data_test = data_test[\"sentence\"].apply(preprocess_text)\n",
        "data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corrector_eval(\n",
        "    corrector: callable,\n",
        "    data: pd.Series,\n",
        "    mutated_data: pd.Series,\n",
        "    edit_distance: int = 1,\n",
        ") -> ConfusionMatrix:\n",
        "    confusion_matrix = ConfusionMatrix()\n",
        "\n",
        "    for sentence, sentence_mutated in tqdm(zip(data, mutated_data), total=len(data)):\n",
        "        sentence_corrected = corrector(sentence_mutated, edit_distance)\n",
        "\n",
        "        confusion_matrix += get_correction_scores(\n",
        "            sentence, sentence_mutated, sentence_corrected\n",
        "        )\n",
        "\n",
        "    return confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 414.44it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1292.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the models\n",
        "N = 1000\n",
        "MUTATTION_RATE = 0.6  # 60% of words are mutated\n",
        "\n",
        "sample_data = data_test.sample(N)\n",
        "mutated_data = sample_data.apply(\n",
        "    lambda x: mutate_random_words_in_text(x, int(MUTATTION_RATE * len(x.split())), 1)\n",
        ")\n",
        "\n",
        "confusion_matrix = corrector_eval(\n",
        "  autocorrect, sample_data, mutated_data, 1\n",
        ")\n",
        "\n",
        "confusion_matrix_norvig = corrector_eval(\n",
        "    norvig_autocorrect, sample_data, mutated_data, 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My Solution\n",
            "        | ---------- | ---------- |\n",
            "        |         TP |       4003 |\n",
            "        |         FP |        149 |\n",
            "        |         TN |       4161 |\n",
            "        |         FN |       1408 |\n",
            "        | ---------- | ---------- |\n",
            "        |   Accuracy |       0.84 |\n",
            "        |  Precision |       0.96 |\n",
            "        |     Recall |       0.74 |\n",
            "        |         F1 |       0.84 |\n",
            "        | ---------- | ---------- |\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(\"My Solution\")\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's Solution\n",
            "        | ---------- | ---------- |\n",
            "        |         TP |       3945 |\n",
            "        |         FP |        149 |\n",
            "        |         TN |       4161 |\n",
            "        |         FN |       1466 |\n",
            "        | ---------- | ---------- |\n",
            "        |   Accuracy |       0.83 |\n",
            "        |  Precision |       0.96 |\n",
            "        |     Recall |       0.73 |\n",
            "        |         F1 |       0.83 |\n",
            "        | ---------- | ---------- |\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(\"Norvig's Solution\")\n",
        "print(confusion_matrix_norvig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, my implementation of context-sensitive spelling correction using a combination of unigram and bigram probabilities has shown marginal improvements over Norvig's.\n",
        "\n",
        "Particularly in terms of recall and F1-score. However, both solutions achieve high accuracy, indicating their practical utility in spelling correction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
