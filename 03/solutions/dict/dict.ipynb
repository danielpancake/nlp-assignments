{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested NER with dictionary and n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterator\n",
    "\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/\"\n",
    "OUT_PATH = \"../../out/dict/\"\n",
    "\n",
    "TRAIN_DATA = os.path.join(DATA_PATH, \"jsonl/train.jsonl\")\n",
    "TEST_DATA = os.path.join(DATA_PATH, \"jsonl/test.jsonl\")\n",
    "\n",
    "SUBMIT_PATH = os.path.join(OUT_PATH, \"test.jsonl\")\n",
    "\n",
    "\n",
    "MAX_NGRAM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path: str) -> Iterator[dict]:\n",
    "    \"\"\"Reads a file in jsonl format\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def write_jsonl(file_path: str, data: Iterator[dict]):\n",
    "    \"\"\"Writes data to a file in jsonl format\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in data:\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "The dictionary is created using the training data. Every token is stored with the collection of labels it is associated with.\n",
    "\n",
    "Every token is stemmed. If token is a list of words, each word is stemmed and the list is joined with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text: str) -> list:\n",
    "    \"\"\"Tokenizes and stems the text\"\"\"\n",
    "    tokenized = list(tokenize(text))\n",
    "\n",
    "    return \" \".join([stemmer.stem(t.text) for t in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(Counter)\n",
    "\n",
    "for sample in read_jsonl(TRAIN_DATA):\n",
    "    for beg, end, label in sample[\"ners\"]:\n",
    "        token = sample[\"sentences\"][beg : end + 1]\n",
    "        token = process_text(token)\n",
    "\n",
    "        model[token][label] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Tokens are n-grams from 1 to `MAX_NGRAM` length.\n",
    "\n",
    "Inference is done by choosing the most common label for each token. If a token is not found in the dictionary, it is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text: str, n: int) -> list:\n",
    "    \"\"\"Extracts ngrams from the text\"\"\"\n",
    "    tokens = list(tokenize(text))\n",
    "\n",
    "    if n <= 1:\n",
    "        return tokens\n",
    "\n",
    "    # Gather ngrams\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tokens[i : i + n]\n",
    "        ngrams.append(ngram)\n",
    "\n",
    "    # Collapse tokens\n",
    "    collapsed_ngrams = []\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        beg = ngram[0].start\n",
    "        end = ngram[-1].stop\n",
    "        ngram_text = \" \".join([_.text for _ in ngram])\n",
    "\n",
    "        collapsed_ngrams.append((beg, end, ngram_text))\n",
    "\n",
    "    return collapsed_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "\n",
    "for sample in read_jsonl(TEST_DATA):\n",
    "    ngrams = [get_ngrams(sample[\"sentences\"], n) for n in range(1, MAX_NGRAM + 1)]\n",
    "\n",
    "    ners = []\n",
    "\n",
    "    for tokens in ngrams:\n",
    "        for beg, end, text in tokens:\n",
    "            text = process_text(text)\n",
    "\n",
    "            if text in model:\n",
    "                label = model[text].most_common(1)[0][0]\n",
    "                ners.append([beg, end - 1, label])\n",
    "\n",
    "    predictions.append({\"id\": sample[\"id\"], \"ners\": ners})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Predictions are saved to ../../out/dict/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "write_jsonl(SUBMIT_PATH, predictions)\n",
    "\n",
    "print(\"Done! Predictions are saved to\", SUBMIT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
